{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargar y enriquecer varios tipos de archivos Azure AI Search\n",
    "\n",
    "En este Jupyter Notebook, creamos y ejecutamos pasos de enriquecimiento para desbloquear contenido buscable en el blob de Azure especificado. Realiza operaciones sobre contenido mixto en Azure Storage, como imágenes y archivos de aplicaciones, utilizando un conjunto de habilidades que analiza y extrae información de texto que se convierte en buscable en Azure Cognitive Search. \n",
    "La muestra de referencia se puede encontrar en [Tutorial: Use Python and AI to generate searchable content from Azure blobs](https://docs.microsoft.com/azure/search/cognitive-search-tutorial-blob-python).\n",
    "\n",
    "En esta demostración vamos a utilizar un contenedor Blob Storage privado (para que podamos imitar un escenario de datalake privado) que tiene ~9.8k PDFs de publicaciones de Ciencias de la Computación del conjunto de datos Arxiv.\n",
    "https://www.kaggle.com/datasets/Cornell-University/arxiv\n",
    "Si desea explorar el conjunto de datos, vaya [AQUÍ](https://console.cloud.google.com/storage/browser/arxiv-dataset/arxiv/cs/pdf?pageState=(%22StorageObjectListTable%22:(%22f%22:%22%255B%255D%22))&prefix=&forceOnObjectsSortingFiltering=false)<br>\n",
    "Nota: Este conjunto de datos se ha copiado en un contenedor blob azure público para esta demostración\n",
    "\n",
    "Aunque aquí sólo se utilizan archivos PDF, esto se puede hacer a una escala mucho mayor y Azure Cognitive Search admite una serie de otros formatos de archivo, incluyendo: Microsoft Office (DOCX/DOC, XSLX/XLS, PPTX/PPT, MSG), HTML, XML, ZIP y archivos de texto sin formato (incluido JSON).\n",
    "Azure Search admite las siguientes fuentes: [Galería de fuentes de datos](https://learn.microsoft.com/EN-US/AZURE/search/search-data-sources-gallery)\n",
    "Este cuaderno crea los siguientes objetos en su servicio de búsqueda:\n",
    "+ fuente de datos (data source)\n",
    "+ índice de búsqueda (search index)\n",
    "+ conjunto de habilidades (skillset)\n",
    "+ indexador (indexer)\n",
    "\n",
    "Este cuaderno llama a las [Search REST APIs](https://docs.microsoft.com/rest/api/searchservice/), pero también puedes utilizar la biblioteca cliente Azure.Search.Documents en el SDK de Azure para Python para realizar los mismos pasos. Consulta este [Python quickstart](https://docs.microsoft.com/azure/search/search-get-started-python) para más detalles.\n",
    "Para ejecutar este cuaderno, ya deberías haber creado los servicios Azure en README. Una vez hecho esto, puedes ejecutar todas las celdas, pero la consulta no devolverá resultados hasta que el indexador haya terminado y el índice de búsqueda esté cargado. \n",
    "Recomendamos ejecutar cada paso y asegurarse de que se completa antes de continuar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cog-search](./images/Cog-Search-Enrich.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"credentials.env\")\n",
    "\n",
    "# Name of the container in your Blob Storage Datasource ( in credentials.env)\n",
    "BLOB_CONTAINER_NAME = \"cybersecurity\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the names for the data source, skillset, index and indexer\n",
    "datasource_name = \"cogsrch-datasource-kiografia\"\n",
    "index_name = \"cogsrch-index-kiografia\"\n",
    "skillset_name = \"cogsrch-skillset-kiografia\"\n",
    "indexer_name = \"cogsrch-indexer-kiografia\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the Payloads header\n",
    "headers = {'Content-Type': 'application/json','api-key': os.environ['AZURE_SEARCH_KEY']}\n",
    "params = {'api-version': os.environ['AZURE_SEARCH_API_VERSION']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data Source (Blob container with the Arxiv CS pdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204\n",
      "True\n",
      ""
     ]
    }
   ],
   "source": [
    "# The following code sends the json paylod to Azure Search engine to create the Datasource\n",
    "\n",
    "datasource_payload = {\n",
    "    \"name\": datasource_name,\n",
    "    \"description\": \"Demo files to demonstrate cognitive search capabilities.\",\n",
    "    \"type\": \"azureblob\",\n",
    "    \"credentials\": {\n",
    "        \"connectionString\": os.environ['BLOB_CONNECTION_STRING']\n",
    "    },\n",
    "    \"dataDeletionDetectionPolicy\" : {\n",
    "        \"@odata.type\" :\"#Microsoft.Azure.Search.NativeBlobSoftDeleteDeletionDetectionPolicy\" # this makes sure that if the item is deleted from the source, it will be deleted from the index\n",
    "    },\n",
    "    \"container\": {\n",
    "        \"name\": BLOB_CONTAINER_NAME\n",
    "    }\n",
    "}\n",
    "r = requests.put(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/datasources/\" + datasource_name,\n",
    "                 data=json.dumps(datasource_payload), headers=headers, params=params)\n",
    "print(r.status_code)\n",
    "print(r.ok)\n",
    "print(os.environ['BLOB_CONNECTION_STRING'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 201 - Creado con éxito\n",
    "- 204 - Sobrescrito correctamente\n",
    "- 40X - Error de autenticación\n",
    "Para obtener información sobre la detección de archivos modificados y eliminados, consulte [AQUÍ](https://learn.microsoft.com/en-us/azure/search/search-howto-index-changed-deleted-blobs?tabs=rest-api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have a 403 code, probably you have a wrong endpoint or key, you can debug by uncomment this\n",
    "# r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crear índice\n",
    "\n",
    "En Azure AI Search, un índice de búsqueda es su contenido de búsqueda, disponible para el motor de búsqueda para indexación, búsqueda de texto completo, búsqueda vectorial, búsqueda híbrida y consultas filtradas. Un índice se define mediante un esquema y se guarda en el servicio de búsqueda, con la importación de datos como segundo paso. Este contenido existe dentro de su servicio de búsqueda, aparte de sus almacenes de datos primarios, lo cual es necesario para los tiempos de respuesta de milisegundos que se esperan en las aplicaciones de búsqueda modernas. Excepto en los escenarios de indexación impulsada por indexadores, el servicio de búsqueda nunca se conecta a los datos de origen ni los consulta.\n",
    "\n",
    "Referencia:\n",
    "https://learn.microsoft.com/en-us/azure/search/search-what-is-an-index\n",
    "\n",
    "Observa a continuación cómo estamos creando un almacén vectorial. En Azure AI Search, un almacén de vectores tiene un esquema de índice que define los campos vectoriales y no vectoriales, una configuración de vectores para los algoritmos que crean el espacio de incrustación y ajustes en las definiciones de campos vectoriales que se utilizan en las solicitudes de consulta. \n",
    "\n",
    "También establecemos una clasificación semántica sobre un conjunto de resultados, promoviendo los resultados semánticamente más relevantes a la parte superior de la pila. También se pueden obtener subtítulos semánticos, con resaltados sobre los términos y frases más relevantes, y respuestas semánticas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Create an index\n",
    "# Queries operate over the searchable fields and filterable fields in the index\n",
    "index_payload = {\n",
    "    \"name\": index_name,\n",
    "    \"vectorSearch\": {\n",
    "        \"algorithms\": [\n",
    "            {\n",
    "                \"name\": \"myalgo\",\n",
    "                \"kind\": \"hnsw\"\n",
    "            }\n",
    "        ],\n",
    "        \"vectorizers\": [\n",
    "            {\n",
    "                \"name\": \"openai\",\n",
    "                \"kind\": \"azureOpenAI\",\n",
    "                \"azureOpenAIParameters\":\n",
    "                {\n",
    "                    \"resourceUri\" : os.environ['AZURE_OPENAI_ENDPOINT'],\n",
    "                    \"apiKey\" : os.environ['AZURE_OPENAI_API_KEY'],\n",
    "                    \"deploymentId\" : os.environ['EMBEDDING_DEPLOYMENT_NAME']\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        \"profiles\": [\n",
    "            {\n",
    "                \"name\": \"myprofile\",\n",
    "                \"algorithm\": \"myalgo\",\n",
    "                \"vectorizer\":\"openai\"\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"semantic\": {\n",
    "        \"configurations\": [\n",
    "            {\n",
    "                \"name\": \"my-semantic-config\",\n",
    "                \"prioritizedFields\": {\n",
    "                    \"titleField\": {\n",
    "                        \"fieldName\": \"title\"\n",
    "                    },\n",
    "                    \"prioritizedContentFields\": [\n",
    "                        {\n",
    "                            \"fieldName\": \"chunk\"\n",
    "                        }\n",
    "                    ],\n",
    "                    \"prioritizedKeywordsFields\": []\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"fields\": [\n",
    "        {\"name\": \"id\", \"type\": \"Edm.String\", \"key\": \"true\", \"analyzer\": \"keyword\", \"searchable\": \"true\", \"retrievable\": \"true\", \"sortable\": \"false\", \"filterable\": \"false\",\"facetable\": \"false\"},\n",
    "        {\"name\": \"ParentKey\", \"type\": \"Edm.String\", \"searchable\": \"true\", \"retrievable\": \"true\", \"facetable\": \"false\", \"filterable\": \"true\", \"sortable\": \"false\"},\n",
    "        {\"name\": \"title\", \"type\": \"Edm.String\", \"searchable\": \"true\", \"retrievable\": \"true\", \"facetable\": \"false\", \"filterable\": \"true\", \"sortable\": \"false\"},\n",
    "        {\"name\": \"name\", \"type\": \"Edm.String\", \"searchable\": \"true\", \"retrievable\": \"true\", \"sortable\": \"false\", \"filterable\": \"false\", \"facetable\": \"false\"},\n",
    "        {\"name\": \"location\", \"type\": \"Edm.String\", \"searchable\": \"true\", \"retrievable\": \"true\", \"sortable\": \"false\", \"filterable\": \"false\", \"facetable\": \"false\"},   \n",
    "        {\"name\": \"chunk\",\"type\": \"Edm.String\", \"searchable\": \"true\", \"retrievable\": \"true\", \"sortable\": \"false\", \"filterable\": \"false\", \"facetable\": \"false\"},\n",
    "        {\n",
    "            \"name\": \"chunkVector\",\n",
    "            \"type\": \"Collection(Edm.Single)\",\n",
    "            \"dimensions\": 1536, # IMPORTANT: Make sure these dimmensions match your embedding model name\n",
    "            \"vectorSearchProfile\": \"myprofile\",\n",
    "            \"searchable\": \"true\",\n",
    "            \"retrievable\": \"true\",\n",
    "            \"filterable\": \"false\",\n",
    "            \"sortable\": \"false\",\n",
    "            \"facetable\": \"false\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "r = requests.put(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/indexes/\" + index_name,\n",
    "                 data=json.dumps(index_payload), headers=headers, params=params)\n",
    "print(r.status_code)\n",
    "print(r.ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if you find an error\n",
    "# r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Capacidades de búsqueda semántica\n",
    "Como puede ver arriba, en la carga útil del índice hay una \"configuración semántica\". ¿En qué consiste?\n",
    "\n",
    "La clasificación semántica es un conjunto de capacidades relacionadas con las consultas que mejoran la calidad de un resultado de búsqueda inicial clasificado por BM25 o RRF para consultas basadas en texto. Cuando se activa en el servicio de búsqueda, la clasificación semántica amplía el proceso de ejecución de consultas de dos maneras:\n",
    "\n",
    "    En primer lugar, añade una clasificación secundaria a un conjunto de resultados inicial que se ha clasificado mediante BM25 o RRF. Esta clasificación secundaria utiliza modelos multilingües de aprendizaje profundo adaptados de Microsoft Bing para promover los resultados semánticamente más relevantes.\n",
    "    \n",
    "    En segundo lugar, extrae y devuelve subtítulos y respuestas en la respuesta, que puede renderizar en una página de búsqueda para mejorar la experiencia de búsqueda del usuario.\n",
    "\n",
    "Para una explicación más detallada y conocer las limitaciones, consulte [AQUÍ](https://learn.microsoft.com/en-us/azure/search/semantic-ranking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crear Skillset - OCR, Text Splitter, AzureOpenAIEmbeddingSkill\n",
    "Ahora tenemos que crear el conjunto de habilidades. Se trata de un conjunto de pasos en los que utilizamos AI Services para enriquecer los documentos extrayendo información, aplicando OCR, dividiendo e incrustando trozos, entre otras habilidades.\n",
    "\n",
    "https://learn.microsoft.com/en-us/azure/search/cognitive-search-working-with-skillsets\n",
    "\n",
    "https://learn.microsoft.com/en-us/azure/search/cognitive-search-predefined-skills"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe a continuación que estamos utilizando IndexProjections. Por defecto, un documento procesado dentro de un conjunto de habilidades se asigna a un único documento en el índice de búsqueda. Esto significa que si realizas un chunking de un texto de entrada y luego realizas enriquecimientos en cada chunk, el resultado en el índice cuando se mapea a través de outputFieldMappings es un array de los enriquecimientos generados. **Con las proyecciones de índice, se define un contexto en el que asignar cada fragmento de datos enriquecidos a su propio documento de búsqueda**. Esto permite aplicar una correspondencia uno a muchos de los datos enriquecidos de un documento al índice de búsqueda.\n",
    "    \n",
    "El parámetro: `\"projectionMode\": \"skipIndexingParentDocuments\"` nos permite omitir la indexación de los documentos padre, y mantener sólo el índice con los chunks y sus vectores.\n",
    "\n",
    "### Ciclo de vida del contenido\n",
    "Si el origen de datos del indizador admite el seguimiento de cambios y la detección de borrados, el proceso de indización puede sincronizar los índices primario (documentos parend) y secundario (chunks) para recoger esos cambios.\n",
    "Cada vez que se ejecuta el indizador y el conjunto de capacidades, las proyecciones del índice se actualizan si el conjunto de capacidades o los datos de origen subyacentes han cambiado. Cualquier cambio recogido por el indexador se propaga a través del proceso de enriquecimiento a las proyecciones del índice, garantizando que los datos proyectados sean una representación actual del contenido de la fuente de datos de origen. Esto le ahorrará semanas de programación y muchos quebraderos de cabeza intentando mantener el contenido sincronizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Create a skillset\n",
    "skillset_payload = {\n",
    "    \"name\": skillset_name,\n",
    "    \"description\": \"e2e Skillset for RAG - Files\",\n",
    "    \"skills\":\n",
    "    [\n",
    "        {\n",
    "            \"@odata.type\": \"#Microsoft.Skills.Vision.OcrSkill\",\n",
    "            \"description\": \"Extract text (plain and structured) from image.\",\n",
    "            \"context\": \"/document/normalized_images/*\",\n",
    "            \"defaultLanguageCode\": \"en\",\n",
    "            \"detectOrientation\": True,\n",
    "            \"inputs\": [\n",
    "                {\n",
    "                  \"name\": \"image\",\n",
    "                  \"source\": \"/document/normalized_images/*\"\n",
    "                }\n",
    "            ],\n",
    "                \"outputs\": [\n",
    "                {\n",
    "                  \"name\": \"text\",\n",
    "                  \"targetName\" : \"images_text\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"@odata.type\": \"#Microsoft.Skills.Text.MergeSkill\",\n",
    "            \"description\": \"Create merged_text, which includes all the textual representation of each image inserted at the right location in the content field. This is useful for PDF and other file formats that supported embedded images.\",\n",
    "            \"context\": \"/document\",\n",
    "            \"insertPreTag\": \" \",\n",
    "            \"insertPostTag\": \" \",\n",
    "            \"inputs\": [\n",
    "                {\n",
    "                  \"name\":\"text\", \"source\": \"/document/content\"\n",
    "                },\n",
    "                {\n",
    "                  \"name\": \"itemsToInsert\", \"source\": \"/document/normalized_images/*/images_text\"\n",
    "                },\n",
    "                {\n",
    "                  \"name\":\"offsets\", \"source\": \"/document/normalized_images/*/contentOffset\"\n",
    "                }\n",
    "            ],\n",
    "            \"outputs\": [\n",
    "                {\n",
    "                  \"name\": \"mergedText\", \n",
    "                  \"targetName\" : \"merged_text\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"@odata.type\": \"#Microsoft.Skills.Text.SplitSkill\",\n",
    "            \"context\": \"/document\",\n",
    "            \"textSplitMode\": \"pages\",  # although it says \"pages\" it actally means chunks, not actual pages\n",
    "            \"maximumPageLength\": 5000, # 5000 characters is default and a good choice\n",
    "            \"pageOverlapLength\": 750,  # 15% overlap among chunks\n",
    "            \"defaultLanguageCode\": \"en\",\n",
    "            \"inputs\": [\n",
    "                {\n",
    "                    \"name\": \"text\",\n",
    "                    \"source\": \"/document/merged_text\"\n",
    "                }\n",
    "            ],\n",
    "            \"outputs\": [\n",
    "                {\n",
    "                    \"name\": \"textItems\",\n",
    "                    \"targetName\": \"chunks\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"@odata.type\": \"#Microsoft.Skills.Text.AzureOpenAIEmbeddingSkill\",\n",
    "            \"description\": \"Azure OpenAI Embedding Skill\",\n",
    "            \"context\": \"/document/chunks/*\",\n",
    "            \"resourceUri\": os.environ['AZURE_OPENAI_ENDPOINT'],\n",
    "            \"apiKey\": os.environ['AZURE_OPENAI_API_KEY'],\n",
    "            \"deploymentId\": os.environ['EMBEDDING_DEPLOYMENT_NAME'],\n",
    "            \"inputs\": [\n",
    "                {\n",
    "                    \"name\": \"text\",\n",
    "                    \"source\": \"/document/chunks/*\"\n",
    "                }\n",
    "            ],\n",
    "            \"outputs\": [\n",
    "                {\n",
    "                    \"name\": \"embedding\",\n",
    "                    \"targetName\": \"vector\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"indexProjections\": {\n",
    "        \"selectors\": [\n",
    "            {\n",
    "                \"targetIndexName\": index_name,\n",
    "                \"parentKeyFieldName\": \"ParentKey\",\n",
    "                \"sourceContext\": \"/document/chunks/*\",\n",
    "                \"mappings\": [\n",
    "                    {\n",
    "                        \"name\": \"title\",\n",
    "                        \"source\": \"/document/title\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"name\",\n",
    "                        \"source\": \"/document/name\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"location\",\n",
    "                        \"source\": \"/document/location\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"chunk\",\n",
    "                        \"source\": \"/document/chunks/*\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"chunkVector\",\n",
    "                        \"source\": \"/document/chunks/*/vector\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"parameters\": {\n",
    "            \"projectionMode\": \"skipIndexingParentDocuments\"\n",
    "        }\n",
    "    },\n",
    "    \"cognitiveServices\": {\n",
    "        \"@odata.type\": \"#Microsoft.Azure.Search.CognitiveServicesByKey\",\n",
    "        \"description\": os.environ['COG_SERVICES_NAME'],\n",
    "        \"key\": os.environ['COG_SERVICES_KEY']\n",
    "    }\n",
    "}\n",
    "\n",
    "r = requests.put(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/skillsets/\" + skillset_name,\n",
    "                 data=json.dumps(skillset_payload), headers=headers, params=params)\n",
    "print(r.status_code)\n",
    "print(r.ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if you find an error\n",
    "# print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crear y ejecutar el Indexador - (ejecuta el pipeline)\n",
    "\n",
    "Los tres componentes que has creado hasta ahora (fuente de datos, conjunto de habilidades, índice) son entradas para un indexador. La creación del indexador en Azure Cognitive Search es el evento que pone en marcha todo el proceso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Create an indexer\n",
    "indexer_payload = {\n",
    "    \"name\": indexer_name,\n",
    "    \"dataSourceName\": datasource_name,\n",
    "    \"targetIndexName\": index_name,\n",
    "    \"skillsetName\": skillset_name,\n",
    "    #\"schedule\" : { \"interval\" : \"PT30M\"}, # How often do you want to check for new content in the data source\n",
    "    \"fieldMappings\": [\n",
    "        {\n",
    "          \"sourceFieldName\" : \"metadata_title\",\n",
    "          \"targetFieldName\" : \"title\"\n",
    "        },\n",
    "        {\n",
    "          \"sourceFieldName\" : \"metadata_storage_name\",\n",
    "          \"targetFieldName\" : \"name\"\n",
    "        },\n",
    "        {\n",
    "          \"sourceFieldName\" : \"metadata_storage_path\",\n",
    "          \"targetFieldName\" : \"location\"\n",
    "        }\n",
    "    ],\n",
    "    \"outputFieldMappings\":[],\n",
    "    \"parameters\":\n",
    "    {\n",
    "        \"maxFailedItems\": -1,\n",
    "        \"maxFailedItemsPerBatch\": -1,\n",
    "        \"configuration\":\n",
    "        {\n",
    "            \"dataToExtract\": \"contentAndMetadata\",\n",
    "            \"imageAction\": \"generateNormalizedImages\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "r = requests.put(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/indexers/\" + indexer_name,\n",
    "                 data=json.dumps(indexer_payload), headers=headers, params=params)\n",
    "print(r.status_code)\n",
    "print(r.ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"@odata.context\":\"https://azure-search-dorak.search.windows.net/$metadata#indexers/$entity\",\"@odata.etag\":\"\\\"0x8DC9AE9E91B5FEE\\\"\",\"name\":\"cogsrch-indexer-kiografia\",\"description\":null,\"dataSourceName\":\"cogsrch-datasource-kiografia\",\"skillsetName\":\"cogsrch-skillset-kiografia\",\"targetIndexName\":\"cogsrch-index-kiografia\",\"disabled\":null,\"schedule\":null,\"parameters\":{\"batchSize\":null,\"maxFailedItems\":-1,\"maxFailedItemsPerBatch\":-1,\"base64EncodeKeys\":null,\"configuration\":{\"dataToExtract\":\"contentAndMetadata\",\"imageAction\":\"generateNormalizedImages\"}},\"fieldMappings\":[{\"sourceFieldName\":\"metadata_title\",\"targetFieldName\":\"title\",\"mappingFunction\":null},{\"sourceFieldName\":\"metadata_storage_name\",\"targetFieldName\":\"name\",\"mappingFunction\":null},{\"sourceFieldName\":\"metadata_storage_path\",\"targetFieldName\":\"location\",\"mappingFunction\":null}],\"outputFieldMappings\":[],\"cache\":null,\"encryptionKey\":null}\n"
     ]
    }
   ],
   "source": [
    "# Uncomment if you find an error\n",
    "#r.text\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "Status: success\n",
      "Items Processed: 3\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Optionally, get indexer status to confirm that it's running\n",
    "try:\n",
    "    r = requests.get(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/indexers/\" + indexer_name +\n",
    "                     \"/status\", headers=headers, params=params)\n",
    "    # pprint(json.dumps(r.json(), indent=1))\n",
    "    print(r.status_code)\n",
    "    print(\"Status:\",r.json().get('lastResult').get('status'))\n",
    "    print(\"Items Processed:\",r.json().get('lastResult').get('itemsProcessed'))\n",
    "    print(r.ok)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"Wait a few seconds until the process starts and run this cell again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referencias\n",
    "\n",
    "- https://learn.microsoft.com/en-us/azure/search/cognitive-search-tutorial-blob\n",
    "- https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/search\n",
    "- https://learn.microsoft.com/en-us/azure/search/search-get-started-vector"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
