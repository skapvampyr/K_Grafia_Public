{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construir nuestro primer bot RAG - Habilidad: hablar con el motor de b칰squeda\n",
    "\n",
    "Ya tenemos todos los bloques para construir nuestro primer Bot que \"hable con mis datos\". Estos bloques son:\n",
    "\n",
    "1) Un motor h칤brido (texto y vector) bien indexado con mis datos en trozos -> Azure AI Search\n",
    "2) Un buen LLM python framework para construir LLM Apps -> LangChain\n",
    "3) Modelos OpenAI GPT de calidad que entiendan el lenguaje y sigan instrucciones -> GPT3.5 y GPT4\n",
    "4) Una base de datos de memoria persistente -> CosmosDB\n",
    "\n",
    "S칩lo nos falta una cosa: **Agentes**.\n",
    "\n",
    "En este Cuaderno introducimos el concepto de Agentes y lo utilizamos para construir nuestro primer bot RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import asyncio\n",
    "from typing import Dict, List\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import Optional, Type\n",
    "\n",
    "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.runnables import ConfigurableField, ConfigurableFieldSpec\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory, CosmosDBChatMessageHistory\n",
    "from langchain.callbacks.manager import AsyncCallbackManagerForToolRun, CallbackManagerForToolRun\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from langchain.tools import BaseTool, StructuredTool, tool\n",
    "\n",
    "#custom libraries that we will use later in the app\n",
    "from common.utils import  GetDocSearchResults_Tool\n",
    "from common.prompts import AGENT_DOCSEARCH_PROMPT\n",
    "\n",
    "from IPython.display import Markdown, HTML, display  \n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"credentials.env\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the ENV variables that Langchain needs to connect to Azure OpenAI\n",
    "os.environ[\"OPENAI_API_VERSION\"] = os.environ[\"AZURE_OPENAI_API_VERSION\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducci칩n: Agentes\n",
    "\n",
    "La implementaci칩n de los agentes se inspira en dos art칤culos: el de [MRKL Systems](https://arxiv.org/abs/2205.00445) (pronunciado \"milagro\" 游땔) y el de [ReAct](https://arxiv.org/abs/2210.03629).\n",
    "\n",
    "Los agentes son una forma de aprovechar la capacidad de los LLM para entender y actuar en funci칩n de las instrucciones. En esencia, un Agente es un LLM al que se le ha dado una indicaci칩n inicial muy inteligente. La indicaci칩n le dice al LLM que descomponga el proceso de respuesta a una consulta compleja en una secuencia de pasos que se resuelven de uno en uno.\n",
    "\n",
    "Los agentes se vuelven realmente interesantes cuando los combinamos con \"expertos\", introducidos en el documento MRKL. Un ejemplo sencillo: un agente puede no tener la capacidad inherente de realizar c치lculos matem치ticos de forma fiable por s칤 mismo. Sin embargo, podemos introducir un experto, en este caso una calculadora, experta en c치lculos matem치ticos. Ahora, cuando necesitemos realizar un c치lculo, el Agente puede llamar al experto en lugar de intentar predecir el resultado por s칤 mismo. Este es en realidad el concepto detr치s de [ChatGPT Pluggins](https://openai.com/blog/chatgpt-plugins).\n",
    "\n",
    "En nuestro caso, para resolver el problema \"C칩mo construyo un bot inteligente que hable con mis datos\", necesitamos este enfoque REACT/MRKL, en el que necesitamos instruir al LLM que necesita usar 'expertos/herramientas' para leer/cargar/entender/interactuar con una fuente de datos en particular.\n",
    "\n",
    "Creemos entonces un Agente que interact칰e con el usuario y utilice una Herramienta para obtener la informaci칩n del Buscador.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Comenzamos definiendo la Herramienta/Experto\n",
    "\n",
    "Las herramientas son funciones que un agente puede invocar. Si no le das al agente acceso a un conjunto correcto de herramientas, nunca podr치 cumplir los objetivos que le asignes. Si no describes bien las herramientas, el agente no sabr치 utilizarlas correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index1_name = \"cogsrch-index-files\"\n",
    "indexes = [index1_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos que convertir el objeto Retreiver en un objeto Tool (\"el experto\"). Echa un vistazo a la herramienta `GetDocSearchResults_Tool` en `utils.py`.\n",
    "\n",
    "Declarar las herramientas que utilizar치 el agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [GetDocSearchResults_Tool(indexes=indexes, k=5, reranker_th=1, sas_token=os.environ['BLOB_SAS_TOKEN'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Definir el LLM a utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPLETION_TOKENS = 1500\n",
    "llm = AzureChatOpenAI(deployment_name=os.environ[\"GPT35_DEPLOYMENT_NAME\"], \n",
    "                      temperature=0.5, max_tokens=COMPLETION_TOKENS, streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Vincular herramientas al LLM\n",
    "\n",
    "Los modelos OpenAI m치s recientes (1106 y posteriores) se han ajustado para detectar cu치ndo se debe llamar a una o m치s funciones y responder con las entradas que se deben pasar a la(s) funci칩n(es). En una llamada a la API, puedes describir funciones y hacer que el modelo elija de forma inteligente la salida de un objeto JSON que contenga argumentos para llamar a estas funciones. El objetivo de las API de las herramientas de OpenAI es devolver de forma m치s fiable llamadas a funciones v치lidas y 칰tiles que lo que puede hacerse utilizando una API gen칠rica de completado de texto o chat.\n",
    "\n",
    "OpenAI denomina **funciones** a la capacidad de invocar una 칰nica funci칩n, y [**herramientas**](https://platform.openai.com/docs/guides/function-calling) a la capacidad de invocar una o m치s funciones.\n",
    "\n",
    "> La API de OpenAI ha dejado obsoletas las funciones en favor de las herramientas. La diferencia entre ambas es que la API de herramientas permite al modelo solicitar que se invoquen varias funciones a la vez, lo que puede reducir los tiempos de respuesta en algunas arquitecturas. Se recomienda utilizar el agente de herramientas para los modelos OpenAI.\n",
    "\n",
    "Hacer que un LLM llame a m칰ltiples herramientas al mismo tiempo puede acelerar enormemente los agentes si hay tareas que se ven asistidas al hacerlo. Afortunadamente, las versiones 1106 y posteriores de los modelos OpenAI soportan llamadas a funciones paralelas, lo que necesitaremos para asegurarnos de que nuestro bot inteligente tiene un buen rendimiento.\n",
    "\n",
    "##### **De ahora en adelante y para el resto de los cuadernos, vamos a utilizar la API de herramientas de OpenAI para llamar a nuestros expertos/herramientas**.\n",
    "\n",
    "Para pasar nuestras herramientas al agente, s칩lo tenemos que formatearlas al [formato de herramientas OpenAI](https://platform.openai.com/docs/api-reference/chat/create) y pasarlas a nuestro modelo. (Al unir las funciones, nos aseguramos de que se pasen cada vez que se invoque el modelo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bind (attach) the tools/functions we want on each LLM call\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "# Let's also add the option to configure in real time the model we want\n",
    "\n",
    "llm_with_tools = llm_with_tools.configurable_alternatives(\n",
    "    ConfigurableField(id=\"model\"),\n",
    "    default_key=\"gpt35\",\n",
    "    gpt4=AzureChatOpenAI(deployment_name=os.environ[\"GPT4_DEPLOYMENT_NAME\"], temperature=0.5, max_tokens=COMPLETION_TOKENS, streaming=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Definir la pregunta del sistema\n",
    "\n",
    "Debido a que OpenAI Function Calling est치 afinado para el uso de herramientas, apenas necesitamos instrucciones sobre c칩mo razonar, o c칩mo el formato de salida. S칩lo tendremos dos variables de entrada: `question` y `agent_scratchpad`. `question` debe ser una cadena que contenga el objetivo del usuario. El `agent_scratchpad` debe ser una secuencia de mensajes que contenga las invocaciones previas a las herramientas del agente y sus correspondientes salidas.\n",
    "\n",
    "Consigue que el prompt use `AGENT_DOCSEARCH_PROMPT` - 춰puedes modificarlo en `prompts.py`! 춰Compru칠balo!\n",
    "Se parece a esto:\n",
    "\n",
    "```python\n",
    "AGENT_DOCSEARCH_PROMPT = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", CUSTOM_CHATBOT_PREFIX  + DOCSEARCH_PROMPT_TEXT),\n",
    "        MessagesPlaceholder(variable_name='history', optional=True),\n",
    "        (\"human\", \"{question}\"),\n",
    "        MessagesPlaceholder(variable_name='agent_scratchpad')\n",
    "    ]\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = AGENT_DOCSEARCH_PROMPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Crear el agente\n",
    "\n",
    "La idea central de los agentes es utilizar un modelo de lenguaje para elegir una secuencia de acciones a realizar. En las cadenas, la secuencia de acciones est치 codificada (en c칩digo). En los agentes, se utiliza un modelo de lenguaje como motor de razonamiento para determinar qu칠 acciones realizar y en qu칠 orden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.format_scratchpad.openai_tools import format_to_openai_tool_messages\n",
    "from langchain.agents.output_parsers.openai_tools import OpenAIToolsAgentOutputParser\n",
    "\n",
    "agent = (\n",
    "    {\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "        \"agent_scratchpad\": lambda x: format_to_openai_tool_messages(x[\"intermediate_steps\"]),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm_with_tools\n",
    "    | OpenAIToolsAgentOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O , lo que es equivalente, LangChain tiene una clase que hace exactamente el c칩digo de la celda de arriba: `create_openai_tools_agent`\n",
    "\n",
    "```python\n",
    "agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "```\n",
    "\n",
    "Crear un agente ejecutor pasando el agente y las herramientas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Darle memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_session_history(session_id: str, user_id: str) -> CosmosDBChatMessageHistory:\n",
    "    cosmos = CosmosDBChatMessageHistory(\n",
    "        cosmos_endpoint=os.environ['AZURE_COSMOSDB_ENDPOINT'],\n",
    "        cosmos_database=os.environ['AZURE_COSMOSDB_NAME'],\n",
    "        cosmos_container=os.environ['AZURE_COSMOSDB_CONTAINER_NAME'],\n",
    "        connection_string=os.environ['AZURE_COMOSDB_CONNECTION_STRING'],\n",
    "        session_id=session_id,\n",
    "        user_id=user_id\n",
    "        )\n",
    "\n",
    "    # prepare the cosmosdb instance\n",
    "    cosmos.prepare_cosmos()\n",
    "    return cosmos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como cosmosDB necesita dos campos (un id y una partici칩n), y RunnableWithMessageHistory toma por defecto un 칰nico identificador para la memoria (session_id), necesitamos utilizar el par치metro `history_factory_config` y definir las m칰ltiples claves para la clase de memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userid_spec = ConfigurableFieldSpec(\n",
    "            id=\"user_id\",\n",
    "            annotation=str,\n",
    "            name=\"User ID\",\n",
    "            description=\"Unique identifier for the user.\",\n",
    "            default=\"\",\n",
    "            is_shared=True,\n",
    "        )\n",
    "session_id = ConfigurableFieldSpec(\n",
    "            id=\"session_id\",\n",
    "            annotation=str,\n",
    "            name=\"Session ID\",\n",
    "            description=\"Unique identifier for the conversation.\",\n",
    "            default=\"\",\n",
    "            is_shared=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_with_chat_history = RunnableWithMessageHistory(\n",
    "    agent_executor,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"history\",\n",
    "    history_factory_config=[userid_spec,session_id]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure the session id and user id\n",
    "random_session_id = \"session\"+ str(random.randint(1, 1000))\n",
    "ramdom_user_id = \"user\"+ str(random.randint(1, 1000))\n",
    "\n",
    "config={\"configurable\": {\"session_id\": random_session_id, \"user_id\": ramdom_user_id}}\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.Ejecutar el Agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "agent_with_chat_history.invoke({\"question\": \"Hi, I'm Pablo Marin. What's yours\"}, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printmd(agent_with_chat_history.invoke(\n",
    "    {\"question\": \"What are some examples of reinforcement learning?\"}, \n",
    "    config=config)[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printmd(agent_with_chat_history.invoke(\n",
    "        {\"question\": \"Interesting, Tell me more about this\"},\n",
    "        config=config)[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printmd(agent_with_chat_history.invoke({\"question\": \"Thhank you!\"}, config=config)[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importante: hay una limitaci칩n de GPT3.5, una vez que empezamos a a침adir preguntas largas, y contextos largos y respuestas minuciosas, o el agente hace m칰ltiples b칰squedas para preguntas de varios pasos, 춰nos quedamos sin espacio!\n",
    "\n",
    "Esto se puede minimizar\n",
    "- Avisos del sistema m치s cortos\n",
    "- Trozos m치s peque침os (menos de los 5000 caracteres por defecto)\n",
    "- Reduciendo topK para traer trozos menos relevantes\n",
    "\n",
    "Sin embargo, en 칰ltima instancia, est치 sacrificando la calidad para que todo funcione con GPT3.5 (modelo m치s barato y m치s r치pido)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A침adamos m치s cosas que hemos aprendido hasta ahora: selecci칩n LLM din치mica de GPT4 y streaming as칤ncrono."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_openai_tools_agent(llm_with_tools.with_config(configurable={\"model\": \"gpt4\"}), tools, prompt) # We select now GPT-4\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=False)\n",
    "agent_with_chat_history = RunnableWithMessageHistory(agent_executor,get_session_history,input_messages_key=\"question\", \n",
    "                                                     history_messages_key=\"history\", history_factory_config=[userid_spec,session_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cuadernos anteriores se utilizaba la funci칩n `.stream()` del ejecutable para transmitir los tokens. Sin embargo, si necesitas transmitir tokens individuales desde el agente o pasos superficiales que ocurren dentro de las herramientas, necesitar치s usar una combinaci칩n de `Callbacks` y `.astream()` O la nueva API `astream_events` (beta).\n",
    "\n",
    "Utilicemos aqu칤 la API astream_events para transmitir los siguientes eventos:\n",
    "\n",
    "    Agente Inicio con entradas\n",
    "    Herramienta Inicio con entradas\n",
    "    Herramienta Final con salidas\n",
    "    Transmitir la respuesta final del agente token a token\n",
    "    Agente Final con salidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION = \"Tell me more about your last answer, search again multiple times and provide a deeper explanation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async for event in agent_with_chat_history.astream_events(\n",
    "    {\"question\": QUESTION}, config=config, version=\"v1\",\n",
    "):\n",
    "    kind = event[\"event\"]\n",
    "    if kind == \"on_chain_start\":\n",
    "        if (event[\"name\"] == \"AgentExecutor\"):\n",
    "            print( f\"Starting agent: {event['name']}\")\n",
    "    elif kind == \"on_chain_end\":\n",
    "        if (event[\"name\"] == \"AgentExecutor\"):  \n",
    "            print()\n",
    "            print(\"--\")\n",
    "            print(f\"Done agent: {event['name']}\")\n",
    "    if kind == \"on_chat_model_stream\":\n",
    "        content = event[\"data\"][\"chunk\"].content\n",
    "        # Empty content in the context of OpenAI means that the model is asking for a tool to be invoked.\n",
    "        # So we only print non-empty content\n",
    "        if content:\n",
    "            print(content, end=\"\")\n",
    "    elif kind == \"on_tool_start\":\n",
    "        print(\"--\")\n",
    "        print(f\"Starting tool: {event['name']} with inputs: {event['data'].get('input')}\")\n",
    "    elif kind == \"on_tool_end\":\n",
    "        print(f\"Done tool: {event['name']}\")\n",
    "        # print(f\"Tool output was: {event['data'].get('output')}\")\n",
    "        print(\"--\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nota: Intenta ejecutar esta 칰ltima pregunta con GPT3.5 y ver치s como te quedas sin espacio para tokens en el LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resumen\n",
    "\n",
    "Acabamos de construir nuestro primer RAG BOT!.\n",
    "\n",
    "- Hemos aprendido que **Agentes + Herramientas son la mejor manera de construir Bots**. <br>\n",
    "- Convertimos el Azure Search retriever en una Herramienta usando la funci칩n `GetDocSearchResults_Tool` en `utils.py`.\n",
    "- Aprendimos sobre la API de eventos (Beta), una forma de transmitir la respuesta de los agentes\n",
    "- Aprendimos que para respuestas completas y de calidad nos quedaremos sin espacio con GPT3.5. GPT4 ser치 entonces necesario.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
