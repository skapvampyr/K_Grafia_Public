{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entendiendo Memoria en LLMs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entendiendo Memoria en LLMs\n",
    "\n",
    "En los cuadernos anteriores, exploramos con éxito cómo los modelos de OpenAI pueden mejorar los resultados de las consultas de Azure AI Search. \n",
    "\n",
    "Sin embargo, aún tenemos que descubrir cómo entablar una conversación con el LLM. Con [Bing Chat](http://chat.bing.com/), por ejemplo, esto es posible, ya que puede entender y hacer referencia a las respuestas anteriores.\n",
    "\n",
    "Existe la idea errónea de que los LLM (Large Language Models) tienen memoria. Esto no es cierto. Aunque poseen conocimientos, no retienen información de preguntas anteriores que se les hayan formulado.\n",
    "\n",
    "En este Cuaderno, nuestro objetivo es ilustrar cómo podemos \"dotar de memoria\" a los LLM de forma eficaz empleando indicaciones y contexto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory, CosmosDBChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables import ConfigurableFieldSpec\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from operator import itemgetter\n",
    "from typing import List\n",
    "\n",
    "from IPython.display import Markdown, HTML, display  \n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "#custom libraries that we will use later in the app\n",
    "from common.utils import CustomAzureSearchRetriever#, get_answer\n",
    "from common.prompts import DOCSEARCH_PROMPT\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"credentials.env\")\n",
    "\n",
    "import logging\n",
    "\n",
    "# Get the root logger\n",
    "logger = logging.getLogger()\n",
    "# Set the logging level to a higher level to ignore INFO messages\n",
    "logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-01-preview\n"
     ]
    }
   ],
   "source": [
    "# Set the ENV variables that Langchain needs to connect to Azure OpenAI\n",
    "os.environ[\"OPENAI_API_VERSION\"] = os.environ[\"AZURE_OPENAI_API_VERSION\"]\n",
    "print(os.environ[\"OPENAI_API_VERSION\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Empecemos por lo básico\n",
    "Vamos a utilizar un ejemplo muy simple para ver si el modelo GPT de Azure OpenAI tiene memoria. De nuevo usaremos langchain para simplificar nuestro código "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION = 'dame los detalles del ticket 3763?'\n",
    "FOLLOW_UP_QUESTION = \"haz una lista\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPLETION_TOKENS = 1000\n",
    "# Create an OpenAI instance\n",
    "llm = AzureChatOpenAI(deployment_name=os.environ[\"GPT35_DEPLOYMENT_NAME\"], \n",
    "                      temperature=0.5, max_tokens=COMPLETION_TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a very simple prompt template, just the question as is:\n",
    "output_parser = StrOutputParser()\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"- **You MUST ONLY answer the question from information contained in the extracted parts (CONTEXT) below**, DO NOT use your prior knowledge. You are an advanced language model specialized in cybersecurity, with deep and up-to-date knowledge on security practices, threats, and technologies. Your goal is to provide precise and useful responses based on the information contained in the ingested documents. This information includes sales and opportunity data from Salesforce, operational details from ServiceDesk+, and data on assets managed by the cybersecurity company\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Lo siento, pero no tengo acceso directo a los detalles específicos del ticket 3763. Sin embargo, si proporcionas información relevante contenida en los documentos que he procesado, puedo ayudarte a analizarla y proporcionarte información basada en esos datos."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's see what the GPT model responds\n",
    "chain = prompt | llm | output_parser\n",
    "response_to_initial_question = chain.invoke({\"input\": QUESTION})\n",
    "display(Markdown(response_to_initial_question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Lamentablemente, no puedo cumplir con tu solicitud ya que no proporcionaste suficiente información sobre el tipo de lista que necesitas. Por favor, proporciona detalles adicionales o haz una pregunta más específica para que pueda ayudarte de manera efectiva."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Now let's ask a follow up question\n",
    "printmd(chain.invoke({\"input\": FOLLOW_UP_QUESTION}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como puedes ver, no recuerda lo que acaba de responder, a veces responde basado solo en el prompt del sistema, o simplemente al azar. Esto prueba que la LLM NO tiene memoria y que necesitamos dar la memoria como un historial de conversación como parte del prompt, así:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_prompt = ChatPromptTemplate.from_template(\n",
    "\"\"\"\n",
    "    {history}\n",
    "    Human: {question}\n",
    "    AI:\n",
    "\"\"\"\n",
    ")\n",
    "chain = hist_prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Conversation_history = \"\"\"\n",
    "Human: {question}\n",
    "AI: {response}\n",
    "\"\"\".format(question=QUESTION, response=response_to_initial_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Lo siento, pero como modelo de lenguaje de inteligencia artificial, no tengo la capacidad de acceder a información específica sobre tickets o documentos. Mi función es generar respuestas basadas en el conocimiento general que he adquirido. Si necesitas información detallada sobre un ticket específico, te recomendaría consultar directamente la fuente de la que proviene el ticket."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "printmd(chain.invoke({\"history\":Conversation_history, \"question\": FOLLOW_UP_QUESTION}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bingo**, así que ya sabemos cómo crear un chatbot utilizando LLMs, sólo tenemos que mantener el estado/historial de la conversación y pasarlo como contexto cada vez."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ahora que entendemos el concepto de memoria añadiendo la historia como contexto, volvamos a nuestro buscador inteligente GPT\n",
    "\n",
    "Del sitio web de Langchain:\n",
    "    \n",
    "Un sistema de memoria necesita soportar dos acciones básicas: lectura y escritura. Recordemos que cada cadena define un núcleo lógico de ejecución que espera ciertas entradas. Algunas de estas entradas vienen directamente del usuario, pero otras pueden venir de la memoria. Una cadena interactuará con su sistema de memoria dos veces en una ejecución dada.\n",
    "\n",
    "    DESPUÉS de recibir las entradas iniciales del usuario pero ANTES de ejecutar la lógica del núcleo, una cadena LEERÁ de su sistema de memoria y aumentará las entradas del usuario.\n",
    "    DESPUÉS de ejecutar la lógica central, pero ANTES de devolver la respuesta, una cadena ESCRIBIRÁ las entradas y salidas de la ejecución actual en la memoria, para que se pueda hacer referencia a ellas en futuras ejecuciones.\n",
    "    \n",
    "Así que este proceso añade retrasos a la respuesta, pero es un retraso necesario :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://python.langchain.com/assets/images/memory_diagram-0627c68230aa438f9b5419064d63cbbc.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "index1_name = \"cogsrch-index-kiografia-csv\"\n",
    "indexes = [index1_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# en la terminal\n",
    "# dar chmod +x al archivo \n",
    "# sudo ./download_odbc_driver.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our custom retriever \n",
    "retriever = CustomAzureSearchRetriever(indexes=indexes, topK=5, reranker_threshold=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si te fijas bien en prompts.py, hay una variable opcional en el `DOCSEARCH_PROMPT` llamada `history`. Ahora es el momento de usarla. Es básicamente un marcador de posición donde inyectaremos la conversación en el prompt para que el LLM sea consciente de ello antes de responder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {} # Our first memory will be a dictionary in memory\n",
    "\n",
    "# We have to define a custom function that takes a session_id and looks somewhere\n",
    "# (in this case in a dictionary in memory) for the conversation\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use our original chain with the retriever but removing the StrOutputParser\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever, \n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"history\": itemgetter(\"history\")\n",
    "    }\n",
    "    | DOCSEARCH_PROMPT\n",
    "    | llm\n",
    ")\n",
    "\n",
    "## Then we pass the above chain to another chain that adds memory to it\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"history\",\n",
    ") | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is where we configure the session id\n",
    "config={\"configurable\": {\"session_id\": \"abc123_KIO_1\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fíjate que estamos añadiendo una variable `history` en la llamada. Esta variable contendrá la historia del chat dentro del prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run bc1118ec-e54e-4748-bc26-fcb7dbf60395 not found for run 9f00278e-4f54-44aa-9863-2af9090d273b. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Lo siento, no tengo información sobre el ticket con el ID 3763 en los datos proporcionados."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "printmd(chain_with_history.invoke({\"question\": QUESTION}, config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 7c15987a-08a7-4e0b-ab35-b0b041172c13 not found for run 50ce17a9-f10e-458e-a218-684fed1268e8. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Tengo información de 2 clientes en total. Los nombres de estos clientes son:\n",
       "1. Bulkmatic\n",
       "2. Procesar"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Remembers\n",
    "printmd(chain_with_history.invoke({\"question\": FOLLOW_UP_QUESTION},config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 60841736-a506-493f-b597-3fd86892be26 not found for run 3dc18f44-8d77-4373-bf11-cf2b6d8218db. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Tengo información de 2 clientes en total. Los nombres de estos clientes son:\n",
       "1. Bulkmatic\n",
       "2. Procesar"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Remembers\n",
    "printmd(chain_with_history.invoke({\"question\": \"Dame la lista completa de clientes\"},config=config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando CosmosDB como memoria persistente\n",
    "\n",
    "En la celda anterior hemos añadido memoria RAM local a nuestro chatbot. Sin embargo, no es persistente, se elimina una vez que la sesión del usuario de la aplicación se termina. Es necesario entonces utilizar una Base de Datos para el almacenamiento persistente de cada una de las conversaciones de los usuarios del bot, no sólo para Análisis y Auditoría, sino también si deseamos proporcionar recomendaciones en el futuro. \n",
    "\n",
    "Aquí almacenaremos el historial de conversaciones en CosmosDB para futuros propósitos de auditoría.\n",
    "Utilizaremos una clase en LangChain llamada CosmosDBChatMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the function to retrieve the conversation\n",
    "\n",
    "def get_session_history(session_id: str, user_id: str) -> CosmosDBChatMessageHistory:\n",
    "    cosmos = CosmosDBChatMessageHistory(\n",
    "        cosmos_endpoint=os.environ['AZURE_COSMOSDB_ENDPOINT'],\n",
    "        cosmos_database=os.environ['AZURE_COSMOSDB_NAME'],\n",
    "        cosmos_container=os.environ['AZURE_COSMOSDB_CONTAINER_NAME'],\n",
    "        connection_string=os.environ['AZURE_COMOSDB_CONNECTION_STRING'],\n",
    "        session_id=session_id,\n",
    "        user_id=user_id\n",
    "        )\n",
    "\n",
    "    # prepare the cosmosdb instance\n",
    "    cosmos.prepare_cosmos()\n",
    "    return cosmos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"history\",\n",
    "    history_factory_config=[\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"user_id\",\n",
    "            annotation=str,\n",
    "            name=\"User ID\",\n",
    "            description=\"Unique identifier for the user.\",\n",
    "            default=\"\",\n",
    "            is_shared=True,\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"session_id\",\n",
    "            annotation=str,\n",
    "            name=\"Session ID\",\n",
    "            description=\"Unique identifier for the conversation.\",\n",
    "            default=\"\",\n",
    "            is_shared=True,\n",
    "        ),\n",
    "    ],\n",
    ") | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is where we configure the session id and user id\n",
    "random_session_id = \"session\"+ str(random.randint(1, 1000))\n",
    "ramdom_user_id = \"user\"+ str(random.randint(1, 1000))\n",
    "\n",
    "config={\"configurable\": {\"session_id\": random_session_id, \"user_id\": ramdom_user_id}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'configurable': {'session_id': 'session248', 'user_id': 'user643'}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run db625af8-a9ec-4a54-a4ef-8e2c4561e4a0 not found for run ab421f59-af78-43c6-9efe-ab406b9b545d. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Poseo información de 4 clientes en total. Los nombres de estos clientes son:\n",
       "1. Abilia\n",
       "2. Bulkmatic\n",
       "3. Grupo Zapata\n",
       "4. Procesar\n",
       "5. Toka"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "printmd(chain_with_history.invoke({\"question\": QUESTION}, config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run a701b6e4-5d08-4bcd-be19-2d74ae63b143 not found for run 32dcc33b-b844-44dc-85ae-be51fdbf18d0. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Poseo información de 5 clientes en total. Los nombres de estos clientes son:\n",
       "\n",
       "1. Abilia\n",
       "2. Bulkmatic\n",
       "3. Grupo Zapata\n",
       "4. Procesar\n",
       "5. Toka"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Remembers\n",
    "printmd(chain_with_history.invoke({\"question\": FOLLOW_UP_QUESTION},config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run afc27b90-382e-447d-9218-2b964d7051ee not found for run 3b0ff0d7-2018-4a93-9817-e2c3bdcf5be5. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "El cliente Bulkmatic tiene un total de 1 ticket en el registro."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Remembers\n",
    "printmd(chain_with_history.invoke(\n",
    "    {\"question\": \"cuántos tickets tiene el cliente Bulkmatic?\"},\n",
    "    config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 65106c54-87dc-42c7-91d7-f0e1b8a9b0f1 not found for run 43c46276-7117-47fe-b0d7-d0184c09d1c3. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "No se proporciona información suficiente en los documentos para determinar el top 5 de tickets de Toka que han tenido el mayor tiempo abierto desde su fecha de creación hasta su fecha de finalización."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    printmd(chain_with_history.invoke(\n",
    "    {\"question\": \"cuál es el top 5 de tickets de Toka que han tenido el mayor tiempo abierto desde su fecha de creación hasta su fecha de finalización?\"},\n",
    "    config=config))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 5340c34c-f499-429a-a32d-cdd6227f669f not found for run f01812a4-6fa1-4266-8835-ede83f4b2aca. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Para el cliente Grupo Zapata, disponemos de las siguientes tecnologías:\n",
       "\n",
       "1. Contivity Vpn Client\n",
       "2. Firewall"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "printmd(chain_with_history.invoke(\n",
    "    {\"question\": \"qué tecnologías tenemos para el cliente Grupo Zapata?\"},\n",
    "    config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run f13e386a-0589-4ba4-b903-7b90d8fbe595 not found for run 8ae53ca5-8dae-4479-bcbe-4a135ed35e44. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "No se proporciona información específica sobre las marcas y modelos de las tecnologías \"Contivity Vpn Client\" y \"Firewall\" para el cliente Grupo Zapata en los documentos disponibles."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "printmd(chain_with_history.invoke(\n",
    "    {\"question\": \"y qué marcas y modelos se tienen para esas tecnologías?\"},\n",
    "    config=config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resumen\n",
    "##### Añadir memoria a nuestra aplicación permite al usuario mantener una conversación, sin embargo esta característica no es algo que venga con el LLM, sino que la memoria es algo que debemos proporcionar al LLM en forma de contexto de la pregunta.\n",
    "\n",
    "Añadimos memoria persistente usando CosmosDB.\n",
    "También podemos notar que la cadena actual que estamos usando es inteligente, pero no tanto. Aunque le hemos dado memoria, busca documentos similares cada vez, independientemente de la entrada. Esto no parece eficiente, pero a pesar de todo, estamos muy cerca de terminar nuestro primer RAG-talk to your data Bot.\n",
    "\n",
    "## <u>Nota Importante</u>:<br>\n",
    "\n",
    "**GPT-3.5-Turbo** puede compararse con un niño de 7 años. Se le pueden dar instrucciones concisas, pero a veces le cuesta seguirlas con precisión (no es demasiado fiable). Además, su \"memoria\" limitada (contexto simbólico) puede dificultar las conversaciones sostenidas. Sus respuestas también son simples, no profundas.\n",
    "\n",
    "El **GPT-4-Turbo** muestra las capacidades de un niño de 10-12 años. Posee una mayor capacidad de razonamiento, sigue sistemáticamente las instrucciones y sus respuestas son mejores. Ha ampliado la retención de memoria (mayor tamaño del contexto) para las instrucciones, y destaca en el seguimiento de las mismas. Sus respuestas son profundas y minuciosas.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
